# exploitation and exploration

from imagegym.env import ENV
import time
import os
import itertools
import numpy as np
import tensorflow as tf
import tensorflow.contrib.layers as layers
from collections import deque
from skimage import color
from skimage.transform import resize
from baselines import deepq
from baselines.common.misc_util import relatively_safe_pickle_dump, pickle_load
from baselines.common import tf_util
from baselines.deepq.replay_buffer import PrioritizedReplayBuffer
from baselines.common.schedules import LinearSchedule, PiecewiseSchedule
from baselines import logger
from baselines.common.tf_util import load_state, save_state

from PIL import Image

bg = "../../data/kite.png"
# ob = "../../data/person.png"
ob = "../../data/bird.png"
env = ENV(bg, ob, True)
im = Image.open(bg)
input_height, input_width = im.size


batch_size = 32
# update_freq = 10000
update_freq = 40
learn_freq = 4
# save_freq = 500000
save_freq = 1200
action_space_size = env.num_actions
NUM_STEPS = 4000000
# NUM_STEPS = 10000
# from 40000 --> 100
# otherwise too much memory used?
replay_memory_size = 1000
replay_alpha = 0.6
replay_beta = 0.4
replay_epsilon = 1e-6
is_load_model = False
watch_flag = False
fps = 30 #frames shown per second when watch_flag == True

log_csv_writer = logger.make_output_format("csv", "logs")
log = logger.Logger("logs", [log_csv_writer])


class TfInput(object):
    def __init__(self, name="(unnamed)"):
        self.name = name

    def get(self):
        raise NotImplemented()

    def make_feed_dict(data):
        raise NotImplemented()

class PlacholderTfInput(TfInput):
    def __init__(self, placeholder):
        super().__init__(placeholder.name)
        self._placeholder = placeholder

    def get(self):
        return self._placeholder

    def make_feed_dict(self, data):
        return {self._placeholder: data}

class Uint8Input(PlacholderTfInput):
    def __init__(self, shape, name=None):
        super().__init__(tf.placeholder(tf.uint8, [None] + list(shape), name=name))
        self._shape = shape
        self._output = tf.cast(super().get(), tf.float32) / 255.0

    def get(self):
        return self._output

from skimage import io

def preprocess_frame(frame):
    # im = resize(color.rgb2gray(frame)[:176, :], (input_height, input_width), mode='constant')
    # im = resize(color.rgb2gray(frame), (input_height, input_width), mode='constant')
    # frame = io.imread(path)
    # resize(color.rgb2gray(frame)[:176, :], (input_height, input_width), mode='constant')
    im = resize(color.rgb2gray(frame), (input_height, input_width), mode='constant')
    return im

def q_function_nn(obs, action_space_size, scope, reuse=False):
    with tf.variable_scope(scope, reuse=reuse):
        out = obs
        out = layers.convolution2d(out, num_outputs=32, kernel_size=8, stride=4, activation_fn=tf.nn.relu)
        out = layers.convolution2d(out, num_outputs=64, kernel_size=4, stride=2, activation_fn=tf.nn.relu)
        out = layers.convolution2d(out, num_outputs=64, kernel_size=3, stride=1, activation_fn=tf.nn.relu)
        conv_out = layers.flatten(out)
        hidden = layers.fully_connected(conv_out, num_outputs=512, activation_fn=tf.nn.relu)
        scores = layers.fully_connected(hidden, num_outputs=action_space_size, activation_fn=None)
        scores_mean = tf.reduce_mean(scores, 1)
        scores = scores - tf.expand_dims(scores_mean, 1)
        return scores

def save_model(dict_state):
    save_state("saved_model/model.ckpt")
    relatively_safe_pickle_dump(dict_state, "saved_model/model_state.pkl.zip", compression=True)

def load_model():
    load_state("saved_model/model.ckpt")
    dict_state = pickle_load("saved_model/model_state.pkl.zip", compression=True)
    return dict_state

def main():
    with tf_util.make_session() as session:
        act_fn, train_fn, target_update_fn, debug_fn = deepq.build_train(
            make_obs_ph=lambda name: Uint8Input([int(input_height), int(input_width)], name=name),
            q_func=q_function_nn,
            num_actions=action_space_size,
            optimizer=tf.train.AdamOptimizer(learning_rate=0.001),
            gamma=0.99,
            grad_norm_clipping=10,
            double_q=False)

        epsilon = PiecewiseSchedule([(0, 1.0),
                                     (400, 1.0), # since we start training at 10000 steps
                                     (800, 0.4),
                                     (2000, 0.2),
                                     (4000, 0.1),
                                     (20000, 0.05)], outside_value=0.01)
        replay_memory = PrioritizedReplayBuffer(replay_memory_size, replay_alpha)
        beta = LinearSchedule(int(NUM_STEPS/4), initial_p=replay_beta, final_p=1.0)
        tf_util.initialize()
        target_update_fn()

        state = env.reset()
        state = preprocess_frame(state)
        watch_train = False
        dq = [] # a queue to store episode rewards
        start_step = 1
        episode = 1
        if is_load_model:
            dict_state = load_model()
            replay_memory = dict_state["replay_memory"]
            dq = dict_state["dq"]
            start_step = dict_state["step"] + 1

        last_step = 0
        iteration = 1
        win_count = 0
        lose_count = 0
        for step in itertools.count(start=start_step):
            action = act_fn(state[np.newaxis], update_eps=epsilon.value(step))[0]
            # print (str(action) + " ", end=' ')
            # print (action)
            state_tplus1, reward, is_finished = env.act(action, step-last_step)
            dq.append(reward)
            if watch_flag:
                # env.render()
                time.sleep(1.0/fps)
            state_tplus1 = preprocess_frame(state_tplus1)
            if is_finished == "win":
                win_count += 1
                is_finished = True
                r = "win"
            elif is_finished == "lose":
                lose_count += 1
                is_finished = True
                r = "lose"
            else:
                is_finished = False

            replay_memory.add(state, action, reward, state_tplus1, float(is_finished))
            state = state_tplus1
            if is_finished:
                ep_reward = sum(dq)
                log.logkv("Steps", step-last_step)
                log.logkv("Episode reward", ep_reward)
                log.logkv("Episode number", episode)
                log.logkv("Results", r)
                log.dumpkvs()
                # print ()
                print ("Step", step-last_step, "Image", step, ". Finished episode", episode, "with reward ", ep_reward, "Results", r)
                print ("================================")
                os.system("mv new_" + str(step-last_step) + ".png logs/new_" + str(iteration) + ".png")
                iteration += 1
                os.system("rm new_*")
                dq = []
                state = preprocess_frame(env.reset())
                episode += 1
                last_step = step

            # if step > 10000 and step % learn_freq == 0:
            if step > 40 and step % learn_freq == 0:
                batch = replay_memory.sample(batch_size, beta=beta.value(step))
                states = batch[0]
                actions = batch[1]
                rewards = batch[2]
                states_tplus1 = batch[3]
                finished_vars = batch[4]
                weights = batch[5]
                state_indeces = batch[6]
                errors = train_fn(states, actions, rewards, states_tplus1, finished_vars, weights)
                priority_order_new = np.abs(errors) + replay_epsilon
                replay_memory.update_priorities(state_indeces, priority_order_new)

            if step % save_freq == 0:
                print("State save", step)
                dict_state = {
                    "step": step,
                    "replay_memory": replay_memory,
                    "dq": dq
                }
                save_model(dict_state)

            if step > NUM_STEPS:
                print("Finished training. Saving model to ./saved_model/model.ckpt")
                dict_state = {
                    "step": step,
                    "replay_memory": replay_memory,
                    "dq": dq
                }
                save_model(dict_state)
                break
main()
